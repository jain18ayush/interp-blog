Title: MNIST-SAE Log 13
Date: 12/29/2024 19:45
Category: Experiment Logs

## Work Done
- 

## Confusions
- The loss is pretty stagnant, will this improve performance for the MNIST? 

## Next Steps

- Train the model on EMNIST rather than MNIST to get more data to actually run the SAEs

- Message Bart Bussman for repo for meta-sae's https://www.alignmentforum.org/inbox/quDmw96SJdzJk8yS3 --> also ask about if this research direction is even logical --> package it up. 
- Need to look at the [impact of sae reconstructions](https://claude.ai/chat/c6e9a978-1aa7-478e-8dc4-7efd1d0cc0a3) on MNIST loss 
    - Essentially tackling cross-entropy loss score 
- Need to [Evaluate my SAEs](https://www.alignmentforum.org/posts/jGG24BzLdYvi9dugm/saebench-a-comprehensive-benchmark-for-sparse-autoencoders)
    - [Full Post](https://www.neuronpedia.org/sae-bench/info) that contains all of the metrics 